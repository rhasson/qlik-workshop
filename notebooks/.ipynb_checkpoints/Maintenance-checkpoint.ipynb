{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3e2ea6",
   "metadata": {},
   "source": [
    "# Managing Iceberg tables\n",
    "\n",
    "In this part of the workshop we'll look at the different ways Iceberg enables you to optimize and maintain your tables.\n",
    "\n",
    "You can learn more in the Iceberg [documentation](https://iceberg.apache.org/docs/latest/spark-procedures/#metadata-management)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01ffdd",
   "metadata": {},
   "source": [
    "### Starting Spark\n",
    "\n",
    "Start Spark and connect to your Lakekeeper Catalog.\n",
    "\n",
    "```NOTE: Only run these if you're starting from scratch. If you ran the Spark notebook, you don't need to execute these two cells```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a57819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'server' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://server:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "MY_NAMESPACE = \"my_db\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fddd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configures the Iceberg catalog (Lakekeeper) and loads the Iceberg library\n",
    "# NOTE: no credentials are being passed. The catalog automatically assigned temp credentials per session\n",
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setMaster('local').setAppName(\"Qlik-Connect-Iceberg-Workshop\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "# Select the \"lakekeeper catalog\" to use in subsequent SQL operations\n",
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90657918",
   "metadata": {},
   "source": [
    "### Create a table and load some data\n",
    "\n",
    "You'll create a table and load some data.  We'll then optimize these files by compacting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2623d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a couple useful libraries\n",
    "\n",
    "!pip install -q requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f975565",
   "metadata": {},
   "source": [
    "We'll download some sample data, NYC Taxi, that we will use to create tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe22e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "### https://data.cityofnewyork.us/NYC-BigApps/Citi-Bike-System-Data/vsnr-94wk\n",
    "\n",
    "r = requests.get('https://gbfs.citibikenyc.com/gbfs/en/station_status.json')\n",
    "station_status = r.json()\n",
    "\n",
    "with open(\"/home/jovyan/work/station_status.json\", \"w\") as f:\n",
    "    for item in station_status['data']['stations']:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n\\r')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f2d2b",
   "metadata": {},
   "source": [
    "Drop any existing tables, if any, so we have a clean slate.\n",
    "\n",
    "Use Spark DataFrames to read the sample data, parse it and then write it out to an Iceberg table called ```stations```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "febf6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE NAMESPACE IF NOT EXISTS {MY_NAMESPACE}').toPandas()\n",
    "spark.sql(f'DROP TABLE IF EXISTS {MY_NAMESPACE}.stations PURGE')\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "          .option(\"header\",True) \\\n",
    "          .option(\"inferschema\",True) \\\n",
    "          .load(\"/home/jovyan/work/station_status.json\")\n",
    "\n",
    "df.repartition(100).write.saveAsTable(f'{MY_NAMESPACE}.stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7e82cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eightd_has_available_keys</th>\n",
       "      <th>is_installed</th>\n",
       "      <th>is_renting</th>\n",
       "      <th>is_returning</th>\n",
       "      <th>last_reported</th>\n",
       "      <th>legacy_id</th>\n",
       "      <th>num_bikes_available</th>\n",
       "      <th>num_bikes_disabled</th>\n",
       "      <th>num_docks_available</th>\n",
       "      <th>num_docks_disabled</th>\n",
       "      <th>num_ebikes_available</th>\n",
       "      <th>num_scooters_available</th>\n",
       "      <th>num_scooters_unavailable</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432869</td>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66dc0dab-0aca-11e7-82f6-3863bb44ef7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432802</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66db2f4c-0aca-11e7-82f6-3863bb44ef7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432770</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66db3f01-0aca-11e7-82f6-3863bb44ef7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432835</td>\n",
       "      <td>3887</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d89dc10f-9be3-45a5-9c3c-06c04c963436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432872</td>\n",
       "      <td>5011</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0109f17a-2378-4500-9648-88bdb31ec612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432786</td>\n",
       "      <td>3677</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>915057d4-edbc-4c73-8d81-dda33d600d58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432854</td>\n",
       "      <td>4448</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28a2436d-a52b-4fa4-874c-e096012a926d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432885</td>\n",
       "      <td>1815968147129520346</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1815968147129520346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432835</td>\n",
       "      <td>1978170169761096104</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1978170169761096104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1743432887</td>\n",
       "      <td>3317</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66ddef2d-0aca-11e7-82f6-3863bb44ef7c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eightd_has_available_keys  is_installed  is_renting  is_returning  \\\n",
       "0                      False             1           1             1   \n",
       "1                      False             1           1             1   \n",
       "2                      False             1           1             1   \n",
       "3                      False             1           1             1   \n",
       "4                      False             1           1             1   \n",
       "5                      False             1           1             1   \n",
       "6                      False             1           1             1   \n",
       "7                      False             1           1             1   \n",
       "8                      False             1           1             1   \n",
       "9                      False             1           1             1   \n",
       "\n",
       "   last_reported            legacy_id  num_bikes_available  \\\n",
       "0     1743432869                  422                    1   \n",
       "1     1743432802                  150                   30   \n",
       "2     1743432770                  244                    0   \n",
       "3     1743432835                 3887                    8   \n",
       "4     1743432872                 5011                   32   \n",
       "5     1743432786                 3677                    4   \n",
       "6     1743432854                 4448                   13   \n",
       "7     1743432885  1815968147129520346                   18   \n",
       "8     1743432835  1978170169761096104                    9   \n",
       "9     1743432887                 3317                   20   \n",
       "\n",
       "   num_bikes_disabled  num_docks_available  num_docks_disabled  \\\n",
       "0                   4                  112                   0   \n",
       "1                   7                   47                   0   \n",
       "2                   4                   47                   0   \n",
       "3                   3                    9                   0   \n",
       "4                   2                    1                   0   \n",
       "5                   0                   13                   0   \n",
       "6                   3                    3                   0   \n",
       "7                   1                    3                   0   \n",
       "8                   1                   25                   0   \n",
       "9                   4                    0                   0   \n",
       "\n",
       "   num_ebikes_available  num_scooters_available  num_scooters_unavailable  \\\n",
       "0                     1                       0                         0   \n",
       "1                     4                       0                         0   \n",
       "2                     0                       0                         0   \n",
       "3                     6                       0                         0   \n",
       "4                     0                       0                         0   \n",
       "5                     0                       0                         0   \n",
       "6                     6                       0                         0   \n",
       "7                     0                       0                         0   \n",
       "8                     4                       0                         0   \n",
       "9                     2                       0                         0   \n",
       "\n",
       "                             station_id  \n",
       "0  66dc0dab-0aca-11e7-82f6-3863bb44ef7c  \n",
       "1  66db2f4c-0aca-11e7-82f6-3863bb44ef7c  \n",
       "2  66db3f01-0aca-11e7-82f6-3863bb44ef7c  \n",
       "3  d89dc10f-9be3-45a5-9c3c-06c04c963436  \n",
       "4  0109f17a-2378-4500-9648-88bdb31ec612  \n",
       "5  915057d4-edbc-4c73-8d81-dda33d600d58  \n",
       "6  28a2436d-a52b-4fa4-874c-e096012a926d  \n",
       "7                   1815968147129520346  \n",
       "8                   1978170169761096104  \n",
       "9  66ddef2d-0aca-11e7-82f6-3863bb44ef7c  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the stations table we just created to ensure data was written\n",
    "\n",
    "spark.sql(f'SELECT * FROM {MY_NAMESPACE}.stations limit 10').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1769622",
   "metadata": {},
   "source": [
    "Check how many files were created.  In this example, we forced Spark to split the data into 100 files, but in the real world this will happen naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f2fe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0       100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'SELECT count(*) FROM lakekeeper.{MY_NAMESPACE}.stations.files').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f0005",
   "metadata": {},
   "source": [
    "### Rewrite data file, aka. compaction\n",
    "\n",
    "Compaction is an important process that combines smalls files into few larger files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9ec9b",
   "metadata": {},
   "source": [
    "We start off compacting our table by looking for 2 or more files with the smallest size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f6b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                       100|                     1|               588160|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                options => map('min-input-files','2', 'rewrite-job-order','bytes-asc')) \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f27263",
   "metadata": {},
   "source": [
    "Inspect the `files` information table again and you'll see that we only have 1 single file now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24076e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'SELECT count(*) FROM lakekeeper.{MY_NAMESPACE}.stations.files').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db3f46",
   "metadata": {},
   "source": [
    "*** Before starting this step, drop the table and recreate it as before so we can test out other compaction scenarios. ***\n",
    "\n",
    "In the following compaction scenario we're sorting the data during compaction. There are bin-packing and sorting using standard ordering or zorder.\n",
    "- Binpacking simply arranges bits to fit more into fewer files.\n",
    "- Sorting organizes rows by sort key so similar data is colocated in the same files making reads more efficient.\n",
    "- Zorder is more complex ordering that comes with its own pros/cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d6c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         0|                     0|                    0|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                strategy => 'sort', \\\n",
    "                sort_order => 'station_id DESC NULLS LAST, legacy_id DESC NULLS LAST') \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b530b9e",
   "metadata": {},
   "source": [
    "Another interesting optimization is to compact only those files that meet a specific filter criteria.  This is helpful when there is large skew in the data and the low cardinality data is not often compacted because it's under the file number of byte size threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ab54c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         0|                     0|                    0|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files(\\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                where => 'is_installed = 1') \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e70d3a",
   "metadata": {},
   "source": [
    "### Expiring snapshots\n",
    "\n",
    "As you already noticed, Iceberg creates lots of snapshots to keep track of changes.  Each snapshot creates numerous manifest files that track everything about files and partitions and schemas.  Each snapshot is also maintains the full table history so you can time travel in queries. However, all of this takes up storage and cost you money.  \n",
    "\n",
    "It's a good practice to expire old snapshots after some period of time or number of snapshots created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b24097",
   "metadata": {},
   "source": [
    "First inspect your `snapshots` information table and lets see which one to expire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b410ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-31 14:58:24.913</td>\n",
       "      <td>7826604710254824548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3://examples/initial-warehouse/0195ecb5-ed6a-...</td>\n",
       "      <td>{'engine-version': '3.5.2', 'total-equality-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-31 14:59:00.377</td>\n",
       "      <td>7431143691552401682</td>\n",
       "      <td>7.826605e+18</td>\n",
       "      <td>replace</td>\n",
       "      <td>s3://examples/initial-warehouse/0195ecb5-ed6a-...</td>\n",
       "      <td>{'engine-version': '3.5.2', 'added-data-files'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id operation  \\\n",
       "0 2025-03-31 14:58:24.913  7826604710254824548           NaN    append   \n",
       "1 2025-03-31 14:59:00.377  7431143691552401682  7.826605e+18   replace   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3://examples/initial-warehouse/0195ecb5-ed6a-...   \n",
       "1  s3://examples/initial-warehouse/0195ecb5-ed6a-...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'engine-version': '3.5.2', 'total-equality-de...  \n",
       "1  {'engine-version': '3.5.2', 'added-data-files'...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'SELECT * FROM lakekeeper.{MY_NAMESPACE}.stations.snapshots').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531117ee",
   "metadata": {},
   "source": [
    "Make sure you update the command below to include the oldest snapshot ID you got from the previous command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5162187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|                     100|                                  0|                                  0|                           1|                           1|                             0|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.expire_snapshots( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                snapshot_ids => ARRAY(CHANGE_ME__SNAPSHOT_ID)) \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd60b86",
   "metadata": {},
   "source": [
    "Inspect the `snapshots` table again and you'll see the old snapshot was removed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
