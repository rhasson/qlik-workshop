{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3e2ea6",
   "metadata": {},
   "source": [
    "# Managing Iceberg tables\n",
    "\n",
    "In this part of the workshop we'll look at the different ways Iceberg enables you to optimize and maintain your tables.\n",
    "\n",
    "You can learn more in the Iceberg [documentation](https://iceberg.apache.org/docs/latest/spark-procedures/#metadata-management)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01ffdd",
   "metadata": {},
   "source": [
    "### Starting Spark\n",
    "\n",
    "Start Spark and connect to your Lakekeeper Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'server' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://server:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "MY_NAMESPACE = \"my_db\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fddd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the Iceberg catalog (Lakekeeper) and loads the Iceberg library\n",
    "# NOTE: no credentials are being passed. The catalog automatically assigned temp credentials per session\n",
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setMaster('local').setAppName(\"Qlik-Connect-Iceberg-Workshop\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "# Select the \"lakekeeper catalog\" to use in subsequent SQL operations\n",
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90657918",
   "metadata": {},
   "source": [
    "### Create a table and load some data\n",
    "\n",
    "You'll create a table and load some data.  We'll then optimize these files by compacting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2623d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the requests library to help us download sample data\n",
    "\n",
    "!pip install -q requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f975565",
   "metadata": {},
   "source": [
    "We'll download some sample data, NYC Taxi, that we will use to create a test table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "### https://data.cityofnewyork.us/NYC-BigApps/Citi-Bike-System-Data/vsnr-94wk\n",
    "\n",
    "r = requests.get('https://gbfs.citibikenyc.com/gbfs/en/station_status.json')\n",
    "station_status = r.json()\n",
    "\n",
    "with open(\"/home/jovyan/work/station_status.json\", \"w\") as f:\n",
    "    for item in station_status['data']['stations']:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n\\r')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f2d2b",
   "metadata": {},
   "source": [
    "Drop any existing tables, if any, so we have a clean slate.\n",
    "\n",
    "Use Spark DataFrames to read the sample data, parse it and then write it out to an Iceberg table called ```stations```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE NAMESPACE IF NOT EXISTS {MY_NAMESPACE}').toPandas()\n",
    "spark.sql(f'DROP TABLE IF EXISTS {MY_NAMESPACE}.stations PURGE')\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "          .option(\"header\",True) \\\n",
    "          .option(\"inferschema\",True) \\\n",
    "          .load(\"/home/jovyan/work/station_status.json\")\n",
    "\n",
    "df.repartition(100).write.saveAsTable(f'{MY_NAMESPACE}.stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the stations table we just created to ensure data was written\n",
    "\n",
    "spark.sql(f'SELECT * FROM {MY_NAMESPACE}.stations limit 10').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1769622",
   "metadata": {},
   "source": [
    "Check how many files were created.  In this example, we forced Spark to split the data into 100 files, but in the real world this will happen naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'SELECT count(*) FROM lakekeeper.{MY_NAMESPACE}.stations.files').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f0005",
   "metadata": {},
   "source": [
    "### Rewrite data file, aka. compaction\n",
    "\n",
    "Compaction is an important process that combines smalls files into few larger files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9ec9b",
   "metadata": {},
   "source": [
    "We start off compacting our table by looking for 2 or more files with the smallest size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                options => map('min-input-files','2', 'rewrite-job-order','bytes-asc')) \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f27263",
   "metadata": {},
   "source": [
    "Inspect the `files` information table again and you'll see that we only have 1 single file now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24076e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'SELECT count(*) FROM lakekeeper.{MY_NAMESPACE}.stations.files').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db3f46",
   "metadata": {},
   "source": [
    "*** Before starting this step, drop the table and recreate it as before so we can test out other compaction scenarios. ***\n",
    "\n",
    "In the following compaction scenario we're sorting the data during compaction. There are bin-packing and sorting using standard ordering or zorder.\n",
    "- Binpacking simply arranges bits to fit more into fewer files.\n",
    "- Sorting organizes rows by sort key so similar data is colocated in the same files making reads more efficient.\n",
    "- Zorder is more complex ordering that comes with its own pros/cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                strategy => 'sort', \\\n",
    "                sort_order => 'station_id DESC NULLS LAST, legacy_id DESC NULLS LAST') \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b530b9e",
   "metadata": {},
   "source": [
    "Another interesting optimization is to compact only those files that meet a specific filter criteria.  This is helpful when there is large skew in the data and the low cardinality data is not often compacted because it's under the file number of byte size threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab54c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.rewrite_data_files(\\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                where => 'is_installed = 1') \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e70d3a",
   "metadata": {},
   "source": [
    "### Expiring snapshots\n",
    "\n",
    "As you already noticed, Iceberg creates lots of snapshots to keep track of changes.  Each snapshot creates numerous manifest files that track everything about files and partitions and schemas.  Each snapshot is also maintains the full table history so you can time travel in queries. However, all of this takes up storage and cost you money.  \n",
    "\n",
    "It's a good practice to expire old snapshots after some period of time or number of snapshots created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b24097",
   "metadata": {},
   "source": [
    "First inspect your `snapshots` information table and lets see which one to expire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'SELECT * FROM lakekeeper.{MY_NAMESPACE}.stations.snapshots').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531117ee",
   "metadata": {},
   "source": [
    "Make sure you update the command below to include the oldest snapshot ID you got from the previous command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5162187",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(f\"CALL lakekeeper.system.expire_snapshots( \\\n",
    "                table => '{MY_NAMESPACE}.stations', \\\n",
    "                snapshot_ids => ARRAY(CHANGE_ME__SNAPSHOT_ID)) \\\n",
    "            \")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd60b86",
   "metadata": {},
   "source": [
    "Inspect the `snapshots` table again and you'll see the old snapshot was removed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
