{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'server' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://server:8181/catalog\"\n",
    "WAREHOUSE = \"demo\"\n",
    "MY_NAMESPACE = \"my_db\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the Iceberg catalog (Lakekeeper) and loads the Iceberg library\n",
    "# NOTE: no credentials are being passed. The catalog automatically assigned temp credentials per session\n",
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster('local').setAppName(\"Qlik-Connect-Iceberg-Workshop\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "# Select the \"lakekeeper catalog\" to use in subsequent SQL operations\n",
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first Iceberg table\n",
    "In this section, we'll create an Iceberg table, load a few rows and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Namespace is a logical grouping of catalog resources, like a database\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {MY_NAMESPACE}\")\n",
    "\n",
    "# Confirm the namespace has been created\n",
    "spark.sql(\"SHOW NAMESPACES\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Iceberg table to represent users, including an ID and a name\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE TABLE {MY_NAMESPACE}.users (\n",
    "                id INT,\n",
    "                name STRING\n",
    "            ) USING ICEBERG\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert some rows into the table.\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            INSERT INTO {MY_NAMESPACE}.users VALUES\n",
    "            (1, 'roy'),\n",
    "            (2, 'ori'),\n",
    "            (3, 'john'),\n",
    "            (4, 'jason'),\n",
    "            (5, 'david'),\n",
    "            (6, 'ajay')\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query your new table and see the rows you inserted\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {MY_NAMESPACE}.users\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg tables under the hood\n",
    "\n",
    "In the following sections we'll look at the structure of an Iceberg table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshots\n",
    "When you query the snapshots information table you'll be able to see the current and previous snapshots of your table.\n",
    "Pay attention to the `summary` column, note that `added-records` equal the number of rows we inserted in the previous statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the snapshots table\n",
    "\n",
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.snapshots\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting a new value\n",
    "Insert a new value to the table. A new snapshot is created and the row is added into a new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"INSERT INTO {MY_NAMESPACE}.users VALUES (7, 'bob');\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second snapshot was created representing the new row we added above.\n",
    "Pay attention under `summary` column to `added-records` which shows 1 and `total-records` which shows 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.snapshots\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating existing values\n",
    "To update an individual value in a table, use the `UPDATE` keyword with an appropriate `WHERE` to identify the row you want to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          UPDATE {MY_NAMESPACE}.users\n",
    "          SET name = 'dave'\n",
    "          WHERE id = 5\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the snapshots table and note the new snapshot that was created represents an `overwrite` operation. In this operation, Iceberg deleted a row, `deleted-record=1` which was the row we updated containing the original values. And added a row `added-record=1` which is the row with the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.snapshots\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the `manifests` table is another way to understand the changes performed on a specific Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.manifests\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting values\n",
    "\n",
    "You can delete values from a table using the `DELETE` keyword and a `WHERE` clause to identify which rows to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DELETE FROM {MY_NAMESPACE}.users WHERE id = 5\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the snapshots table and confirm a new one has been created\n",
    "\n",
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.snapshots\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when you exectue the following query, no results are returned. This means that no delete files where created when rows were updated or deleted.\n",
    "Kind of strage no?  Well in fact that's because the table is by default configured to Copy On Write. This mode of operation automatically merges the delete files with data files. This is ideal for batch workloads with flexible latency, but is far less ideal for streaming, near real time use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.all_delete_files\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with MoR and CoW tables\n",
    "\n",
    "MoR tables require the query engine to merge data and delete files on read. You can periodically compact these files to make the query engine's life easier.\n",
    "CoW tables merges the data and delete files when the rows are written. It requires more IO on write, but far less on read.\n",
    "\n",
    "Spark allows you to configure MoR or CoW for either `delete`, `update` or `merge` operations. This gives you flexibility to control how your tables should be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by changing the default mode of operation that Spark uses to write and update Iceberg tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER TABLE {MY_NAMESPACE}.users SET TBLPROPERTIES ('write.update.mode'='merge-on-read')\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets update a row and see how our table reacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "            UPDATE {MY_NAMESPACE}.users\n",
    "            SET name='bobby'\n",
    "            WHERE id = 6\n",
    "          \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if any delete files were created.  Remember, previously no delete files where created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.all_delete_files\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the `manifests` table also shows that Iceberg created a specific manifest file to track the delete file, along with manifests to track the data files.\n",
    "You can tell by looking at the `content` column. `0` means manifest tracking data files and `1` means manifest tracking delete files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM lakekeeper.{MY_NAMESPACE}.users.manifests\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
